base_model: google/gemma-3-27b-it
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Quantization & LoRA

load_in_4bit: true
adapter: qlora
peft_use_dora: true
peft_use_rslora: true
lora_r: 32
lora_alpha: 32 # Consider using 16 to prevent overwriting pre-trained weights
lora_dropout: 0.05
lora_target_linear: true

# Hardware & Speed

flash_attention: true
xformers_attention: false
bf16: true
fp16: false
plugins:
  - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin

# Context & Batching

sample_packing: true
eval_sample_packing: true
pad_to_sequence_len: true
sequence_len: 12000

# Gradient accumulation = 16 * 1 micro_batch * 3 GPUs = Global Batch 48

micro_batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Optimizer & Training

learning_rate: 0.000005
optimizer: paged_adamw_8bit
lr_scheduler: cosine
warmup_ratio: 0.1
eval_steps: 10
save_steps: 10
num_epochs: 1
logging_steps: 1
early_stopping_patience: 4
# load_best_model_at_end: true
output_dir: ./outputs/bananabernini_27b

# Dataset Configuration
shuffle_merged_datasets: true
datasets:
  - path: training_data.jsonl
    type: chat_template
    chat_template: tokenizer_default
    field_messages: messages
    message_property_mappings:
      role: from
      content: value

test_datasets:
  - path: test_data.jsonl
    type: chat_template
    chat_template: tokenizer_default
    field_messages: messages
    message_property_mappings:
      role: from
      content: value

# Data Loading

dataloader_prefetch_factor: 4
dataloader_num_workers: 4
dataloader_pin_memory: true

# Multi-GPU settings

deepspeed: deepspeed_configs/zero2.json

# Regularization
# neftune_noise_alpha: 5

trust_remote_code: true

strict: false
